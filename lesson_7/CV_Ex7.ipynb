{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. warm up\n",
    "\n",
    "Load the 'cereals_query' and 'cereals_frame' images.\n",
    "\n",
    "Calculate SIFT keypoints, and try to find best matches. Use these matches to estimate the Homography parameters (using RANSAC). Mark the transformed box of the query image on top of the 'cereals_frame' image.\n",
    "\n",
    "useful links:\n",
    "\n",
    "https://www.learnopencv.com/homography-examples-using-opencv-python-c/\n",
    "\n",
    "http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html#feature-homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('cereals_frame.jpg', 0)\n",
    "img_c = cv2.imread('cereals_frame.jpg')\n",
    "q = cv2.imread('cereals_query.jpg', 0)\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create(nfeatures = 0, sigma=1.6, edgeThreshold=10, contrastThreshold=0.04)\n",
    "\n",
    "kpts_img, des_img = sift.detectAndCompute(img,None)\n",
    "img_kpts_output = cv2.drawKeypoints(img, kpts_img, outImage=None, flags=4)\n",
    "kpts_q, des_q = sift.detectAndCompute(q,None)\n",
    "q_kpts_output = cv2.drawKeypoints(q, kpts_q, outImage=None, flags=4)\n",
    "\n",
    "bfm = cv2.BFMatcher()\n",
    "\n",
    "# for knn match\n",
    "knnm1 = bfm.knnMatch(des_img,des_q,k=2)\n",
    "knn_img_to_q = bfm.knnMatch(des_q, des_img, k=2)\n",
    "\n",
    "# max 20 matches\n",
    "max_matches = 20\n",
    "\n",
    "# for knn match\n",
    "matches_img_q = list(filter(lambda m : m[0].distance < 0.4 * m[1].distance, knnm1))[:max_matches]\n",
    "\n",
    "good_matches = [m[0] for m in knn_img_to_q if m[0].distance < 0.4 * m[1].distance]\n",
    "#matches_img_q1 = sorted(bfm.match(des_img,des_q1),key=lambda x:x.distance)\n",
    "\n",
    "\n",
    "\n",
    "#for knn match\n",
    "matches_img_q_res = cv2.drawMatchesKnn(img,kpts_img,q,kpts_q,matches_img_q,outImg=None,\n",
    "                          flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "                        \n",
    "\n",
    "#matches_img_q1_res = cv2.drawMatches(img,kpts_img,q1,kpts_q1,matches_img_q1[:20],outImg=None,\n",
    "#                                     flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imshow('img', matches_img_q_res)\n",
    "key_pressed = False\n",
    "while not key_pressed:\n",
    "    k = cv2.waitKey(33)\n",
    "    if k != -1:  # if no key was pressed, -1 is returned\n",
    "        key_pressed = True\n",
    "cv2.destroyWindow('img')\n",
    "\n",
    "if len(good_matches)>10:\n",
    "    src_pts = np.float32([ kpts_q[m.queryIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kpts_img[m.trainIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
    "    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    h,w = q.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,H)\n",
    "    img_c_with_border = cv2.polylines(img_c,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('img', img_c_with_border)\n",
    "key_pressed = False\n",
    "while not key_pressed:\n",
    "    k = cv2.waitKey(33)\n",
    "    if k != -1:  # if no key was pressed, -1 is returned\n",
    "        key_pressed = True\n",
    "cv2.destroyWindow('img')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. lets make a movie!\n",
    "\n",
    "Load each and every frame in the 'cereals_movie' file, and repeat the same analysis as before. For each input frame, generate an output frame with the transormed box of the query image on top (check that you have enough 'good matched' before). Generate an output movie from all the output frames. \n",
    "\n",
    "Submit both the `.ipynb` notebook and your movie file.\n",
    "\n",
    "In case the movie frames are too big, you can rescale them to a lower resolution, before performing your analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'xfeatures2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dac442ba3438>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecognize_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'movie'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-dac442ba3438>\u001b[0m in \u001b[0;36mrecognize_frame\u001b[1;34m(img, q)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrecognize_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIFT_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgeThreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrastThreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.04\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mkpts_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdes_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'xfeatures2d'"
     ]
    }
   ],
   "source": [
    "def recognize_frame(img, q):\n",
    "    \n",
    "    sift = cv2.xfeatures2d.SIFT_create(nfeatures = 0, sigma=1.6, edgeThreshold=10, contrastThreshold=0.04)\n",
    "\n",
    "    kpts_img, des_img = sift.detectAndCompute(img,None)\n",
    "    kpts_q, des_q = sift.detectAndCompute(q,None)\n",
    "\n",
    "    bfm = cv2.BFMatcher()\n",
    "\n",
    "    # for knn match\n",
    "    knn_img_to_q = bfm.knnMatch(des_q, des_img, k=2)\n",
    "\n",
    "    # max 20 matches\n",
    "    max_matches = 20\n",
    "\n",
    "\n",
    "    good_matches = [m[0] for m in knn_img_to_q if m[0].distance < 0.4 * m[1].distance][:max_matches]\n",
    "    #matches_img_q1 = sorted(bfm.match(des_img,des_q1),key=lambda x:x.distance)\n",
    "    if len(good_matches) > 10:\n",
    "        src_pts = np.float32([ kpts_q[m.queryIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kpts_img[m.trainIdx].pt for m in good_matches ]).reshape(-1,1,2)\n",
    "        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "        h,w = q.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "        dst = cv2.perspectiveTransform(pts,H)\n",
    "        img = cv2.polylines(img,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "    \n",
    "          \n",
    "    return img\n",
    "\n",
    "\n",
    "# play and save a movie\n",
    "video_attr = ('cereals_movie','mp4v', 20.0, 540, 960, '.mp4')\n",
    "cap = cv2.VideoCapture('cereals_movie.mp4')\n",
    "fourcc = cv2.VideoWriter_fourcc(*video_attr[1])\n",
    "video_name = '{0}_{1}_{2}_{3}_{4}_output{5}'.format(*video_attr)\n",
    "out = cv2.VideoWriter(video_name, fourcc, video_attr[2], (video_attr[3],video_attr[4]))\n",
    "q = cv2.imread('cereals_query.jpg', 0)\n",
    "\n",
    "grayscale = False\n",
    "\n",
    "cv2.imshow('query', q)\n",
    "key_pressed = False\n",
    "while not key_pressed:\n",
    "    k = cv2.waitKey(33)\n",
    "    if k != -1:  # if no key was pressed, -1 is returned\n",
    "        key_pressed = True\n",
    "cv2.destroyWindow('query')\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==False:\n",
    "        break\n",
    "    if grayscale:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    frame = recognize_frame(frame, q)\n",
    "    cv2.imshow('movie',frame)\n",
    "    out.write(frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
